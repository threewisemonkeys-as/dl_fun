{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "pacman_screen_as_state.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/threewisemonkeys-as/dl_fun/blob/master/atari_env_rl/pacman_screen_as_state.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHhYqaKycyEH",
        "colab_type": "code",
        "outputId": "552eb276-fcb4-43a1-96b3-c36124b8f1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import collections\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYwQFrTEcyFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('MsPacman-v0').unwrapped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ee_U-bIcyGB",
        "colab_type": "text"
      },
      "source": [
        "### Define NN architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZpmQDLkcyGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    \n",
        "    def __init__(self, h, w, outputs):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        \n",
        "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
        "            return (size - kernel_size) // stride + 1\n",
        "        \n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "        linear_input_size = convw * convh * 32\n",
        "        self.head = nn.Linear(linear_input_size, outputs)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPubWYzcyGP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Define function to get screenshot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVP_FMWucyGR",
        "colab_type": "code",
        "outputId": "ddec4f3d-d5ac-45b1-c470-9b393bc08d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "proccess_screen = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Lambda(lambda img: T.functional.crop(img, 0, 5, 170, 150)),\n",
        "    T.Grayscale(),\n",
        "    T.Resize([100, 100]),\n",
        "    T.ToTensor(),\n",
        "    T.Lambda(lambda img: img.unsqueeze(0).to(device)),\n",
        "])\n",
        "\n",
        "def view_screen(screen, title='Screen'):\n",
        "    plt.figure()\n",
        "    plt.clf()\n",
        "    plt.imshow(proccess_screen(screen).squeeze(0).squeeze(0).cpu().numpy(), cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "view_screen(env.reset(), title='Example extracted screen')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7vAkVuc1RAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7TD-5OD16dJ",
        "colab_type": "code",
        "outputId": "71489d8c-94db-4a65-8b57-09056fb129bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "proccess_screen(env.reset()).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 100, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kIYrM7KcyGj",
        "colab_type": "text"
      },
      "source": [
        "### Initialize NNs and other utitlities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "603NEer9cyGn",
        "colab_type": "code",
        "outputId": "bea5ab9a-c64a-4917-f967-011050baea15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Custom tuple to store each transition\n",
        "Transition = collections.namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "# Get screen dimensions\n",
        "_, _, screen_height, screen_width = proccess_screen(env.reset()).shape\n",
        "\n",
        "# Instatiate policy and target NNs.\n",
        "policy_net = DQN(screen_height, screen_width, env.action_space.n).to(device)\n",
        "target_net = DQN(screen_height, screen_width, env.action_space.n).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DQN(\n",
              "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(2, 2))\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
              "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
              "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (head): Linear(in_features=2592, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2kxBJ9CcyGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model(memory, optimizer, BATCH_SIZE, GAMMA, losses):\n",
        "    \n",
        "    # Dont update model until replay memory has enough elements\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    \n",
        "    # Get random sample batch from replay memory\n",
        "    transitions = random.sample(memory, BATCH_SIZE)\n",
        "    \n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "        \n",
        "    \n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
        "                                  device=device, dtype=torch.uint8)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    \n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    \n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # Clip gradients \n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrps4nWtcyGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_episodes, optimizer,\n",
        "          REPLAY_MEMORY_SIZE, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TARGET_UPDATE,\n",
        "          verbose=False, animation=False):\n",
        "    \n",
        "    # Memory to store previous experiences\n",
        "    memory = collections.deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "    \n",
        "    # Tracking variables\n",
        "    steps_done = 0\n",
        "    step_wise_scores = []\n",
        "    episode_wise_scores = []\n",
        "    losses = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        print(f\"Running Episode {i_episode + 1}/{num_episodes} ...\")\n",
        "        \n",
        "        # Initialize the environment and state\n",
        "        state = proccess_screen(env.reset())\n",
        "        episode_wise_scores.append(0)\n",
        "\n",
        "        # Play single game\n",
        "        for t in itertools.count():           \n",
        "            \n",
        "            # Select and perform an action using epsilon greedy\n",
        "            steps_done += 1\n",
        "\n",
        "            if EPS_DECAY is not None:\n",
        "              EPS = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "            else:\n",
        "              EPS = EPS_START - (EPS_START - EPS_END) * (i_episode / num_episodes)\n",
        "\n",
        "            if random.random() > EPS:\n",
        "                with torch.no_grad():\n",
        "                    action = policy_net(state).max(1)[1].view(1, 1)\n",
        "            else:\n",
        "                 action = torch.tensor([[random.randrange(env.action_space.n)]], \n",
        "                                       device=device, dtype=torch.long)\n",
        "\n",
        "            # Take action and record the reward from the environment\n",
        "            current_screen, reward, done, _ = env.step(action.item())\n",
        "            step_wise_scores.append(reward)\n",
        "            episode_wise_scores[-1] += reward\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            next_state = proccess_screen(current_screen)\n",
        "            \n",
        "            # Store the transition in memory\n",
        "            memory.append(Transition(state, action, next_state, reward, done))\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            # Update the target network, copying all weights and biases in DQN\n",
        "            if steps_done % TARGET_UPDATE == 0:\n",
        "                target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "            # Perform one step of the optimization (on the target network)\n",
        "            optimize_model(memory, optimizer, BATCH_SIZE, GAMMA, losses)\n",
        "            \n",
        "            # Render the gameplay\n",
        "            if animation:\n",
        "                env.render()\n",
        "            \n",
        "            # Log progress periodically\n",
        "            if (t + 1) % 100 == 0 and verbose:\n",
        "                print(f\" - Reward after {t + 1} steps is {episode_wise_scores[-1]}\")\n",
        "            \n",
        "            # Break from loop if done\n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "        print(f\"> Score for Episode {i_episode + 1} = {episode_wise_scores[-1]} after completing {t + 1} steps. | Running Average = {(sum(episode_wise_scores) / len(episode_wise_scores)):.1f}\\n\")\n",
        "        \n",
        "    if animation:\n",
        "        env.close()\n",
        "        \n",
        "    print('\\nTraining complete!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3LIYqk2cyHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_net(num_episodes, verbose=False, animation=False):\n",
        "    \n",
        "    episode_wise_scores = []\n",
        "    step_wise_scores = []\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        print(f\"Running {i_episode + 1}/{num_episodes} ...\")\n",
        "        \n",
        "        # Initialize the environment and state\n",
        "        state = proccess_screen(env.reset())\n",
        "        episode_wise_scores.append(0)\n",
        "\n",
        "        # Play single game\n",
        "        for t in itertools.count():\n",
        "            \n",
        "            # Select action\n",
        "            with torch.no_grad():\n",
        "                action = policy_net(state).max(1)[1].view(1, 1)\n",
        "                    \n",
        "            # Take action and record the reward from the environment\n",
        "            current_screen, reward, done, _ = env.step(action.item())\n",
        "            step_wise_scores.append(reward)\n",
        "            episode_wise_scores[-1] += reward\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            next_state = proccess_screen(current_screen)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            \n",
        "            # Render the gameplay\n",
        "            if animation:\n",
        "                env.render()\n",
        "            \n",
        "            # Log progress periodically\n",
        "            if (t + 1) % 100 == 0 and verbose:\n",
        "                print(f\" - Reward after {t} steps is {episode_wise_scores[-1]}\")\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        print(f\"> Score for Episode {i_episode} = {episode_wise_scores[-1]} | Running Average = {(sum(episode_wise_scores) / len(episode_wise_scores)):.1f}\\n\")\n",
        "\n",
        "    if animation:\n",
        "        env.close()\n",
        "    \n",
        "    print(f\"\\nComplete | Average score = {sum(episode_wise_scores) / len(episode_wise_scores)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U00UVrIcyHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_random(num_episodes, verbose=False, animation=False):\n",
        "    episode_wise_scores = []\n",
        "    step_wise_scores = []\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        print(f\"Running {i_episode + 1}/{num_episodes} ...\")\n",
        "        \n",
        "        # Initialize the environment and state\n",
        "        state = proccess_screen(env.reset())\n",
        "        episode_wise_scores.append(0)\n",
        "\n",
        "        # Play single game\n",
        "        for t in itertools.count():\n",
        "            \n",
        "            # Select action\n",
        "            with torch.no_grad():\n",
        "                action = random.randrange(env.action_space.n)\n",
        "                    \n",
        "            # Take action and record the reward from the environment\n",
        "            current_screen, reward, done, _ = env.step(action)\n",
        "            step_wise_scores.append(reward)\n",
        "            episode_wise_scores[-1] += reward\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            next_state = proccess_screen(current_screen)\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "            \n",
        "            # Render the gameplay\n",
        "            if animation:\n",
        "                env.render()\n",
        "            \n",
        "            # Log progress periodically\n",
        "            if (t + 1) % 100 == 0 and verbose:\n",
        "                print(f\" - Reward after {t} steps is {episode_wise_scores[-1]}\")\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        print(f\"> Score for Episode {i_episode} = {episode_wise_scores[-1]} | Running Average = {(sum(episode_wise_scores) / len(episode_wise_scores)):.1f}\\n\")\n",
        "\n",
        "    if animation:\n",
        "        env.close()\n",
        "    \n",
        "    print(f\"\\nComplete | Average score = {sum(episode_wise_scores) / len(episode_wise_scores)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgSxCmPqiFGg",
        "colab_type": "code",
        "outputId": "6b93280e-fe10-4394-d3a4-5153ffbb9aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(num_episodes=10,\n",
        "      optimizer=optim.RMSprop(policy_net.parameters()),\n",
        "      REPLAY_MEMORY_SIZE=10_000,\n",
        "      BATCH_SIZE=128,\n",
        "      GAMMA=0.95,\n",
        "      EPS_START=0.9,\n",
        "      EPS_END=0.1,\n",
        "      EPS_DECAY=None,\n",
        "      TARGET_UPDATE=50,\n",
        "      verbose=True,\n",
        "      animation=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Episode 1/10 ...\n",
            " - Reward after 100 steps is 10.0\n",
            " - Reward after 200 steps is 50.0\n",
            " - Reward after 300 steps is 100.0\n",
            " - Reward after 400 steps is 120.0\n",
            " - Reward after 500 steps is 160.0\n",
            "> Score for Episode 1 = 190.0 after completing 582 steps. | Running Average = 190.0\n",
            "\n",
            "Running Episode 2/10 ...\n",
            " - Reward after 100 steps is 10.0\n",
            " - Reward after 200 steps is 50.0\n",
            " - Reward after 300 steps is 50.0\n",
            " - Reward after 400 steps is 110.0\n",
            " - Reward after 500 steps is 110.0\n",
            "> Score for Episode 2 = 160.0 after completing 586 steps. | Running Average = 175.0\n",
            "\n",
            "Running Episode 3/10 ...\n",
            " - Reward after 100 steps is 20.0\n",
            " - Reward after 200 steps is 80.0\n",
            " - Reward after 300 steps is 100.0\n",
            " - Reward after 400 steps is 160.0\n",
            " - Reward after 500 steps is 210.0\n",
            " - Reward after 600 steps is 220.0\n",
            " - Reward after 700 steps is 240.0\n",
            "> Score for Episode 3 = 240.0 after completing 736 steps. | Running Average = 196.7\n",
            "\n",
            "Running Episode 4/10 ...\n",
            " - Reward after 100 steps is 10.0\n",
            " - Reward after 200 steps is 100.0\n",
            " - Reward after 300 steps is 110.0\n",
            " - Reward after 400 steps is 120.0\n",
            " - Reward after 500 steps is 120.0\n",
            " - Reward after 600 steps is 210.0\n",
            "> Score for Episode 4 = 250.0 after completing 680 steps. | Running Average = 210.0\n",
            "\n",
            "Running Episode 5/10 ...\n",
            " - Reward after 100 steps is 10.0\n",
            " - Reward after 200 steps is 70.0\n",
            " - Reward after 300 steps is 80.0\n",
            " - Reward after 400 steps is 100.0\n",
            " - Reward after 500 steps is 130.0\n",
            " - Reward after 600 steps is 170.0\n",
            " - Reward after 700 steps is 210.0\n",
            "> Score for Episode 5 = 210.0 after completing 712 steps. | Running Average = 210.0\n",
            "\n",
            "Running Episode 6/10 ...\n",
            " - Reward after 100 steps is 30.0\n",
            " - Reward after 200 steps is 100.0\n",
            " - Reward after 300 steps is 140.0\n",
            " - Reward after 400 steps is 180.0\n",
            " - Reward after 500 steps is 200.0\n",
            " - Reward after 600 steps is 200.0\n",
            "> Score for Episode 6 = 210.0 after completing 665 steps. | Running Average = 210.0\n",
            "\n",
            "Running Episode 7/10 ...\n",
            " - Reward after 100 steps is 20.0\n",
            " - Reward after 200 steps is 90.0\n",
            " - Reward after 300 steps is 120.0\n",
            " - Reward after 400 steps is 160.0\n",
            " - Reward after 500 steps is 220.0\n",
            "> Score for Episode 7 = 220.0 after completing 559 steps. | Running Average = 211.4\n",
            "\n",
            "Running Episode 8/10 ...\n",
            " - Reward after 100 steps is 30.0\n",
            " - Reward after 200 steps is 110.0\n",
            " - Reward after 300 steps is 110.0\n",
            " - Reward after 400 steps is 190.0\n",
            " - Reward after 500 steps is 260.0\n",
            "> Score for Episode 8 = 260.0 after completing 547 steps. | Running Average = 217.5\n",
            "\n",
            "Running Episode 9/10 ...\n",
            " - Reward after 100 steps is 20.0\n",
            " - Reward after 200 steps is 90.0\n",
            " - Reward after 300 steps is 90.0\n",
            " - Reward after 400 steps is 110.0\n",
            " - Reward after 500 steps is 130.0\n",
            " - Reward after 600 steps is 190.0\n",
            "> Score for Episode 9 = 190.0 after completing 645 steps. | Running Average = 214.4\n",
            "\n",
            "Running Episode 10/10 ...\n",
            " - Reward after 100 steps is 20.0\n",
            " - Reward after 200 steps is 60.0\n",
            " - Reward after 300 steps is 150.0\n",
            " - Reward after 400 steps is 210.0\n",
            "> Score for Episode 10 = 220.0 after completing 485 steps. | Running Average = 215.0\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT4eW_Dzg1CL",
        "colab_type": "code",
        "outputId": "43b8fef2-3577-4504-b5e1-37e6abc7077f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "test_net(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4319a8d50a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_net' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0BYbJ_9g4HU",
        "colab_type": "code",
        "outputId": "526db467-da1e-42d8-cd6a-79f15dce1d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "test_random(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 1/10 ...\n",
            "> Score for Episode 0 = 310.0 | Running Average = 310.0\n",
            "\n",
            "Running 2/10 ...\n",
            "> Score for Episode 1 = 160.0 | Running Average = 235.0\n",
            "\n",
            "Running 3/10 ...\n",
            "> Score for Episode 2 = 160.0 | Running Average = 210.0\n",
            "\n",
            "Running 4/10 ...\n",
            "> Score for Episode 3 = 200.0 | Running Average = 207.5\n",
            "\n",
            "Running 5/10 ...\n",
            "> Score for Episode 4 = 150.0 | Running Average = 196.0\n",
            "\n",
            "Running 6/10 ...\n",
            "> Score for Episode 5 = 230.0 | Running Average = 201.7\n",
            "\n",
            "Running 7/10 ...\n",
            "> Score for Episode 6 = 190.0 | Running Average = 200.0\n",
            "\n",
            "Running 8/10 ...\n",
            "> Score for Episode 7 = 340.0 | Running Average = 217.5\n",
            "\n",
            "Running 9/10 ...\n",
            "> Score for Episode 8 = 240.0 | Running Average = 220.0\n",
            "\n",
            "Running 10/10 ...\n",
            "> Score for Episode 9 = 190.0 | Running Average = 217.0\n",
            "\n",
            "\n",
            "Complete | Average score = 217.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIaYJL5vxkQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "def download_policy_net():\n",
        "  torch.save(policy_net.state_dict(), 'checkpoint.pth')\n",
        "  files.download('checkpoint.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKghb-iix2j2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}